{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom sklearn.preprocessing import  LabelEncoder\nfrom tqdm.auto import tqdm\nimport random\nimport ast\nimport os\nimport dill\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom transformers import AutoTokenizer, AutoConfig,AutoModel\nimport json\nimport torch\nimport torch.nn.functional as F\nfrom ast import literal_eval\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import optim\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport time","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:38.721390Z","iopub.execute_input":"2022-12-29T21:05:38.721854Z","iopub.status.idle":"2022-12-29T21:05:39.721492Z","shell.execute_reply.started":"2022-12-29T21:05:38.721766Z","shell.execute_reply":"2022-12-29T21:05:39.720474Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(0)\nrandom.seed(0)\nnp.random.seed(0)\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:39.723527Z","iopub.execute_input":"2022-12-29T21:05:39.724428Z","iopub.status.idle":"2022-12-29T21:05:39.736979Z","shell.execute_reply.started":"2022-12-29T21:05:39.724386Z","shell.execute_reply":"2022-12-29T21:05:39.735761Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7fb07936f6b0>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load dataframes\n\n##### Let's load all the datasets from the competition:","metadata":{}},{"cell_type":"code","source":"DIR = '/kaggle/input/'","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:39.738684Z","iopub.execute_input":"2022-12-29T21:05:39.739045Z","iopub.status.idle":"2022-12-29T21:05:39.744266Z","shell.execute_reply.started":"2022-12-29T21:05:39.739009Z","shell.execute_reply":"2022-12-29T21:05:39.743357Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"features = pd.read_csv(DIR+\"nbme-score-clinical-patient-notes/features.csv\")\npatient_notes = pd.read_csv(DIR+\"nbme-score-clinical-patient-notes/patient_notes.csv\")\ntest = pd.read_csv(DIR+\"nbme-score-clinical-patient-notes/test.csv\")\ntrain= pd.read_csv(DIR+\"nbme-score-clinical-patient-notes/train.csv\")\nsample_submission= pd.read_csv(DIR+\"nbme-score-clinical-patient-notes/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:39.746799Z","iopub.execute_input":"2022-12-29T21:05:39.747569Z","iopub.status.idle":"2022-12-29T21:05:40.076681Z","shell.execute_reply.started":"2022-12-29T21:05:39.747501Z","shell.execute_reply":"2022-12-29T21:05:40.075684Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"\n##### The features.csv file contain all the features:","metadata":{}},{"cell_type":"code","source":"features.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.078039Z","iopub.execute_input":"2022-12-29T21:05:40.079072Z","iopub.status.idle":"2022-12-29T21:05:40.091756Z","shell.execute_reply.started":"2022-12-29T21:05:40.079034Z","shell.execute_reply":"2022-12-29T21:05:40.090561Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   feature_num  case_num                                       feature_text\n0            0         0  Family-history-of-MI-OR-Family-history-of-myoc...\n1            1         0                 Family-history-of-thyroid-disorder\n2            2         0                                     Chest-pressure\n3            3         0                              Intermittent-symptoms\n4            4         0                                        Lightheaded","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_num</th>\n      <th>case_num</th>\n      <th>feature_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>Family-history-of-thyroid-disorder</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>Chest-pressure</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>Intermittent-symptoms</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>Lightheaded</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"##### The patient_notes.csv fie contains all the patient history notes:","metadata":{}},{"cell_type":"code","source":"patient_notes.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.093734Z","iopub.execute_input":"2022-12-29T21:05:40.094164Z","iopub.status.idle":"2022-12-29T21:05:40.106033Z","shell.execute_reply.started":"2022-12-29T21:05:40.094110Z","shell.execute_reply":"2022-12-29T21:05:40.104987Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   pn_num  case_num                                         pn_history\n0       0         0  17-year-old male, has come to the student heal...\n1       1         0  17 yo male with recurrent palpitations for the...\n2       2         0  Dillon Cleveland is a 17 y.o. male patient wit...\n3       3         0  a 17 yo m c/o palpitation started 3 mos ago; \\...\n4       4         0  17yo male with no pmh here for evaluation of p...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pn_num</th>\n      <th>case_num</th>\n      <th>pn_history</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>17-year-old male, has come to the student heal...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>17 yo male with recurrent palpitations for the...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>Dillon Cleveland is a 17 y.o. male patient wit...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>a 17 yo m c/o palpitation started 3 mos ago; \\...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>17yo male with no pmh here for evaluation of p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":" As we can see the training set contains the annotation feature that is comprised of strings of medical features detected in the patient history and the corresponding location that is the target feature that our model aims to detect. The id feature is simply the result of concatenating the case_num, pn_num and feature_num features values toghether. \n","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.107522Z","iopub.execute_input":"2022-12-29T21:05:40.107948Z","iopub.status.idle":"2022-12-29T21:05:40.121430Z","shell.execute_reply.started":"2022-12-29T21:05:40.107913Z","shell.execute_reply":"2022-12-29T21:05:40.120356Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"          id  case_num  pn_num  feature_num  \\\n0  00016_000         0      16            0   \n1  00016_001         0      16            1   \n2  00016_002         0      16            2   \n3  00016_003         0      16            3   \n4  00016_004         0      16            4   \n\n                                 annotation              location  \n0          ['dad with recent heart attcak']           ['696 724']  \n1             ['mom with \"thyroid disease']           ['668 693']  \n2                        ['chest pressure']           ['203 217']  \n3      ['intermittent episodes', 'episode']  ['70 91', '176 183']  \n4  ['felt as if he were going to pass out']           ['222 258']  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>case_num</th>\n      <th>pn_num</th>\n      <th>feature_num</th>\n      <th>annotation</th>\n      <th>location</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00016_000</td>\n      <td>0</td>\n      <td>16</td>\n      <td>0</td>\n      <td>['dad with recent heart attcak']</td>\n      <td>['696 724']</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00016_001</td>\n      <td>0</td>\n      <td>16</td>\n      <td>1</td>\n      <td>['mom with \"thyroid disease']</td>\n      <td>['668 693']</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00016_002</td>\n      <td>0</td>\n      <td>16</td>\n      <td>2</td>\n      <td>['chest pressure']</td>\n      <td>['203 217']</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00016_003</td>\n      <td>0</td>\n      <td>16</td>\n      <td>3</td>\n      <td>['intermittent episodes', 'episode']</td>\n      <td>['70 91', '176 183']</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00016_004</td>\n      <td>0</td>\n      <td>16</td>\n      <td>4</td>\n      <td>['felt as if he were going to pass out']</td>\n      <td>['222 258']</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"The test.csv dataset contains only information about the case number, the patient number and the feature number. The feature number will have to be identified in the corresponding text.\n","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.123030Z","iopub.execute_input":"2022-12-29T21:05:40.123408Z","iopub.status.idle":"2022-12-29T21:05:40.133732Z","shell.execute_reply.started":"2022-12-29T21:05:40.123363Z","shell.execute_reply":"2022-12-29T21:05:40.132563Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"          id  case_num  pn_num  feature_num\n0  00016_000         0      16            0\n1  00016_001         0      16            1\n2  00016_002         0      16            2\n3  00016_003         0      16            3\n4  00016_004         0      16            4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>case_num</th>\n      <th>pn_num</th>\n      <th>feature_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00016_000</td>\n      <td>0</td>\n      <td>16</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00016_001</td>\n      <td>0</td>\n      <td>16</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00016_002</td>\n      <td>0</td>\n      <td>16</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00016_003</td>\n      <td>0</td>\n      <td>16</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00016_004</td>\n      <td>0</td>\n      <td>16</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"##### This is an example of the format of the submission on Kaggle:","metadata":{}},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.135201Z","iopub.execute_input":"2022-12-29T21:05:40.136181Z","iopub.status.idle":"2022-12-29T21:05:40.148174Z","shell.execute_reply.started":"2022-12-29T21:05:40.136021Z","shell.execute_reply":"2022-12-29T21:05:40.147596Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"          id         location\n0  00016_000            0 100\n1  00016_001              NaN\n2  00016_002  200 250;300 400\n3  00016_003              NaN\n4  00016_004           75 110","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>location</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00016_000</td>\n      <td>0 100</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00016_001</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00016_002</td>\n      <td>200 250;300 400</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00016_003</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00016_004</td>\n      <td>75 110</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"##### Merging the train and test datasets with the patient notes and features datasets:","metadata":{}},{"cell_type":"code","source":"train = train.merge(patient_notes,on=['case_num','pn_num']).merge(features,on=['case_num','feature_num'])\ntest = test.merge(patient_notes,on=['case_num','pn_num']).merge(features,on=['case_num','feature_num'])","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.152094Z","iopub.execute_input":"2022-12-29T21:05:40.152951Z","iopub.status.idle":"2022-12-29T21:05:40.186598Z","shell.execute_reply.started":"2022-12-29T21:05:40.152915Z","shell.execute_reply":"2022-12-29T21:05:40.185739Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.190065Z","iopub.execute_input":"2022-12-29T21:05:40.190369Z","iopub.status.idle":"2022-12-29T21:05:40.202787Z","shell.execute_reply.started":"2022-12-29T21:05:40.190337Z","shell.execute_reply":"2022-12-29T21:05:40.201721Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"          id  case_num  pn_num  feature_num                        annotation  \\\n0  00016_000         0      16            0  ['dad with recent heart attcak']   \n1  00041_000         0      41            0                                []   \n2  00046_000         0      46            0          ['father: heart attack']   \n3  00082_000         0      82            0                     ['Father MI']   \n4  00100_000         0     100            0                        ['Dad-MI']   \n\n      location                                         pn_history  \\\n0  ['696 724']  HPI: 17yo M presents with palpitations. Patien...   \n1           []  17 Y/O M CAME TO THE CLINIC C/O HEART POUNDING...   \n2  ['824 844']  Mr. Cleveland is a 17yo M who was consented by...   \n3  ['622 631']  17 yo M w/ no cardiac or arrhythmia PMH presen...   \n4  ['735 741']  HPI: Dillon Cleveland is an otherwise healthy ...   \n\n                                        feature_text  \n0  Family-history-of-MI-OR-Family-history-of-myoc...  \n1  Family-history-of-MI-OR-Family-history-of-myoc...  \n2  Family-history-of-MI-OR-Family-history-of-myoc...  \n3  Family-history-of-MI-OR-Family-history-of-myoc...  \n4  Family-history-of-MI-OR-Family-history-of-myoc...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>case_num</th>\n      <th>pn_num</th>\n      <th>feature_num</th>\n      <th>annotation</th>\n      <th>location</th>\n      <th>pn_history</th>\n      <th>feature_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00016_000</td>\n      <td>0</td>\n      <td>16</td>\n      <td>0</td>\n      <td>['dad with recent heart attcak']</td>\n      <td>['696 724']</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00041_000</td>\n      <td>0</td>\n      <td>41</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>17 Y/O M CAME TO THE CLINIC C/O HEART POUNDING...</td>\n      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00046_000</td>\n      <td>0</td>\n      <td>46</td>\n      <td>0</td>\n      <td>['father: heart attack']</td>\n      <td>['824 844']</td>\n      <td>Mr. Cleveland is a 17yo M who was consented by...</td>\n      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00082_000</td>\n      <td>0</td>\n      <td>82</td>\n      <td>0</td>\n      <td>['Father MI']</td>\n      <td>['622 631']</td>\n      <td>17 yo M w/ no cardiac or arrhythmia PMH presen...</td>\n      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00100_000</td>\n      <td>0</td>\n      <td>100</td>\n      <td>0</td>\n      <td>['Dad-MI']</td>\n      <td>['735 741']</td>\n      <td>HPI: Dillon Cleveland is an otherwise healthy ...</td>\n      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.204592Z","iopub.execute_input":"2022-12-29T21:05:40.205062Z","iopub.status.idle":"2022-12-29T21:05:40.218524Z","shell.execute_reply.started":"2022-12-29T21:05:40.205027Z","shell.execute_reply":"2022-12-29T21:05:40.217557Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"          id  case_num  pn_num  feature_num  \\\n0  00016_000         0      16            0   \n1  00016_001         0      16            1   \n2  00016_002         0      16            2   \n3  00016_003         0      16            3   \n4  00016_004         0      16            4   \n\n                                          pn_history  \\\n0  HPI: 17yo M presents with palpitations. Patien...   \n1  HPI: 17yo M presents with palpitations. Patien...   \n2  HPI: 17yo M presents with palpitations. Patien...   \n3  HPI: 17yo M presents with palpitations. Patien...   \n4  HPI: 17yo M presents with palpitations. Patien...   \n\n                                        feature_text  \n0  Family-history-of-MI-OR-Family-history-of-myoc...  \n1                 Family-history-of-thyroid-disorder  \n2                                     Chest-pressure  \n3                              Intermittent-symptoms  \n4                                        Lightheaded  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>case_num</th>\n      <th>pn_num</th>\n      <th>feature_num</th>\n      <th>pn_history</th>\n      <th>feature_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00016_000</td>\n      <td>0</td>\n      <td>16</td>\n      <td>0</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00016_001</td>\n      <td>0</td>\n      <td>16</td>\n      <td>1</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n      <td>Family-history-of-thyroid-disorder</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00016_002</td>\n      <td>0</td>\n      <td>16</td>\n      <td>2</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n      <td>Chest-pressure</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00016_003</td>\n      <td>0</td>\n      <td>16</td>\n      <td>3</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n      <td>Intermittent-symptoms</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00016_004</td>\n      <td>0</td>\n      <td>16</td>\n      <td>4</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n      <td>Lightheaded</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{}},{"cell_type":"markdown","source":"##### For the choice of the epochs, lr and batch size we followed the guidance of the BERT paper and tried the following combinations: \n-  ##### Batch size: 16, 32\n-  ##### Learning rate (Adam): 5e-5, 3e-5, 2e-5\n-  ##### Number of epochs: 2, 3, ","metadata":{}},{"cell_type":"markdown","source":"The rules of the competition do not consent to use the internet when doing the final submission. Thus, when this code runs on Kaggle the BERT model is taken from a dataset on Kaggle in which it was loaded and so the model_name in the hyperparameters is set to: '../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased'. \nOtherwise, when internet can be used the model_name is simply 'bert-base-uncased'. \n","metadata":{}},{"cell_type":"code","source":"hyperparameters = {\n    'max_length': 512,\n    'padding': 'max_length',\n    'return_offsets_mapping': True,\n    'truncation': 'only_second',\n    'model_name': '../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased',\n    'dropout': 0.1,\n    'lr': [5e-5, 3e-5, 2e-5],\n    'val_size': 0.2,\n    'seed': 999,\n    'batch_size': [16,32],\n    'epochs': [2,3,4]\n}","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.220171Z","iopub.execute_input":"2022-12-29T21:05:40.220557Z","iopub.status.idle":"2022-12-29T21:05:40.226511Z","shell.execute_reply.started":"2022-12-29T21:05:40.220521Z","shell.execute_reply":"2022-12-29T21:05:40.225488Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# BERT Tokenizer","metadata":{}},{"cell_type":"code","source":"DATA_PATH = DIR + 'nbmemodel'\nDATA_EXISTS = os.path.exists(DATA_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.227844Z","iopub.execute_input":"2022-12-29T21:05:40.228430Z","iopub.status.idle":"2022-12-29T21:05:40.235114Z","shell.execute_reply.started":"2022-12-29T21:05:40.228396Z","shell.execute_reply":"2022-12-29T21:05:40.234202Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"In our project we will fine-tune the bert-base-uncased model. The Bert model requires a certain input format so we first use the Bert tokenizer to achieve that trough the AutoTokenizer class. By quoting the Hugging Face tutorial, \"AutoClasses are here so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary, that will directly create a class of the relevant architecture\".. The Bert model requires a certain input format so we first use the Bert tokenizer by using the AutoTokenizer class. By quoting the Hugging Face tutorial, \"AutoClasses are here so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary, that will directly create a class of the relevant architecture\".","metadata":{}},{"cell_type":"code","source":"if DATA_EXISTS:\n    tokenizer = AutoTokenizer.from_pretrained(DATA_PATH+\"/my_tokenizer/\",normalization=True)\n    config = AutoConfig.from_pretrained(DATA_PATH+\"/my_tokenizer/config.json\")\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(hyperparameters['model_name'],normalization=True) \n    config = AutoConfig.from_pretrained(hyperparameters['model_name'])\n    tokenizer.save_pretrained('/my_tokenizer')\n    config.save_pretrained('/my_tokenizer')","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.236746Z","iopub.execute_input":"2022-12-29T21:05:40.237101Z","iopub.status.idle":"2022-12-29T21:05:40.271662Z","shell.execute_reply.started":"2022-12-29T21:05:40.237064Z","shell.execute_reply":"2022-12-29T21:05:40.270819Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Label Encoding","metadata":{}},{"cell_type":"markdown","source":"##### There are 144 unique classes, i.e. unique medical features. As we can see however the numbers are not ordered. ","metadata":{}},{"cell_type":"code","source":"EMPTY =  -1\nCLASSES = [EMPTY,]+features.feature_num.unique().tolist()\nprint(\"Unique classes: \", CLASSES)\nprint(\"\\nNumber of unique classes: \", len(CLASSES))","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.273003Z","iopub.execute_input":"2022-12-29T21:05:40.273422Z","iopub.status.idle":"2022-12-29T21:05:40.280577Z","shell.execute_reply.started":"2022-12-29T21:05:40.273384Z","shell.execute_reply":"2022-12-29T21:05:40.279348Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Unique classes:  [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 700, 701, 702, 703, 704, 705, 706, 707, 708, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916]\n\nNumber of unique classes:  144\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Each feature number is encoded into unique labels that go from 1 to 143 plus the label 0 that signals the \"empty feature\". This will be used when the patient_history text will be tokenized and the scope of the model will be to classify each token, if the label is 0 it means that the corresponding token is not part of any medical feature while if the label is a number between 1-143 that token is a part of the medical feature. ","metadata":{}},{"cell_type":"code","source":"if DATA_EXISTS:\n    label_encoder = dill.load(open(DATA_PATH+\"/label_encoder.dill\",'rb'))\nelse:\n    # label_encoder\n    label_encoder = LabelEncoder()\n    # Encode labels\n    label_encoder.fit(CLASSES)\n    dill.dump(label_encoder,open('label_encoder.dill','wb'))\n    \ntrain['TARGET']= label_encoder.transform(train['feature_num'])\ntest['TARGET']= label_encoder.transform(test['feature_num'])\nN_CLASSES = len(label_encoder.classes_)\nEMPTY_IDX = label_encoder.transform([EMPTY,])[0]\nprint(f\"Empty label: {EMPTY_IDX}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.281809Z","iopub.execute_input":"2022-12-29T21:05:40.282532Z","iopub.status.idle":"2022-12-29T21:05:40.297394Z","shell.execute_reply.started":"2022-12-29T21:05:40.282488Z","shell.execute_reply":"2022-12-29T21:05:40.296153Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Empty label: 0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.1.3 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Ausiliary functions","metadata":{}},{"cell_type":"code","source":"def decode_location(locations):\n    \"\"\"This function transform the location features from a string representation of a list to a list of tuples\"\"\"\n    for x in [\"[\",\"]\",\"'\"]:\n        locations = locations.replace(x,'')\n    locations = locations.replace(',',';')\n    locations = locations.split(\";\")\n    res = []\n    for location in locations:\n        if location:\n            x,y = location.split()\n            res.append((int(x),int(y)))\n    return sorted(res,key=lambda x:x[0])\n\ndef decode_metrics(locations):\n    res = []\n    for location in ast.literal_eval(locations):\n        if location:\n            for loc in location.split(';'):\n                x,y = loc.split()\n                res.append(np.arange(int(x), int(y)))\n    res = np.array(res, dtype = object)\n    try:\n        res = np.concatenate(res)\n        res = np.array(list(set(res)))\n    except:\n        pass\n    return res\n\ndef decode_position(pos):\n    \"\"\"This function transforms the predicted position to the format required in the Kaggle submission \"\"\"\n    return \";\".join([\" \".join(np.array(p).astype(str)) for p in pos])\n\n\ndef translate(preds,targets_to_row_ids,offsets):\n    \"\"\"This function takes the predicitons and for each target feature in the test dataset \n    checks whether that feature is predicted somewhere in the sequence.\n    If a target feature is detected in the prediction vector, it returns the characters positions of the feature.\"\"\"\n    all_ids = []\n    all_pos = []\n\n    for k in range(len(preds)):\n        offset = offsets[k]\n        pred = preds[k]\n        \n        targets_to_ids = targets_to_row_ids[k]\n        prediction = {targets_to_ids[t]:[] for t in targets_to_ids}\n        i = 0\n        while i<hyperparameters['max_length']:\n            label = pred[i]\n        \n            if label == EMPTY_IDX:\n                i += 1\n                continue\n            if label in targets_to_ids:\n                key = targets_to_ids[label]\n                start = offset[i][0]\n                while i<hyperparameters['max_length']:\n                    if pred[i] != label:\n                        break\n                    else:\n                        end = max(offset[i])\n                    i += 1\n                if  end == 0:\n                    break\n                prediction[key].append((start,end))\n            else:\n                i+=1\n        for key in prediction:\n            all_ids.append(key)\n            all_pos.append(decode_position(prediction[key]))\n    df = pd.DataFrame({\n        \"id\":all_ids,\n        \"location\": all_pos\n    })\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.299100Z","iopub.execute_input":"2022-12-29T21:05:40.299654Z","iopub.status.idle":"2022-12-29T21:05:40.315587Z","shell.execute_reply.started":"2022-12-29T21:05:40.299619Z","shell.execute_reply":"2022-12-29T21:05:40.314662Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the inputs for the model","metadata":{}},{"cell_type":"markdown","source":"In the following cell we use the encode_plus function from the Hugging Face transformers library. This takes in input a text, in this case the patient history and it tokenizes it. Therefore, each word is mapped to a unique index that corresponds to a word in the tokenizer vocabulary. Of course, not all words are present in the Bert vocabulary so some words are broken into subwords. Moreover, there are some special characters that are added: [SEP] and [CLS]. The [SEP] token is added to indicate the end of a sentence and the other one is a special classification token. Bert has also two additional constraints: each sentence must be padded or truncated to a fixed length and the maximum sentence length is 512 tokens. For our task, we use the maximum possible length since there are no reasons to truncate the text and padding will not cause any issues since the dataset is quite small. The [PAD] characters are thus special tokens that indicate that those tokens are not 'expected' tokens but rather tokens added just to align the sequences to the same length. \nThe encoder_plus returns two inputs that will be directly used by our model:\n- the input_ids vector that is the vector of indexes described before in which each token is mapped to a index in the Bert vocabulary\n- the attention masks vector which explicitly differentiate real tokens from [PAD] tokens.\n\nMoreover, by setting the parameter \"return_offsets_mapping\" this function also returns a vector that specify the characters start position and end position for each token. This is very useful to build the ground truth vector because it can be checked whether each token is part of an annotation in the patient history. If that is the case, it will be assigned the matching feautre to the ground truth vector entry corresponding to that token, otherwise it will be assigned a zero, the empty feauture. This procedure is done for the unique patient history texts, so the ground truth vector has shape (1000, 512) which when hot-encoded becomes (1000, 512, 144).","metadata":{}},{"cell_type":"code","source":"if not DATA_EXISTS:\n    sequences, labels, masks,  offsets_list  = [], [], [], []\n    row_ids = []\n    targets = []\n\n    for group in tqdm(train.groupby('pn_num')): #the training set is grouped by the patient number. There are 1000 unique patient numbers\n        group_df = group[1] #to extract the grouped df corresponding to each pn_num, group[0] instead returns the pn_num\n        pn_history  = group_df.iloc[0].pn_history\n        tokens = tokenizer.encode_plus(pn_history, max_length=hyperparameters['max_length'], padding='max_length', truncation=True, return_offsets_mapping=True)\n        sequence = tokens['input_ids'] #token embedding, each token is mapped to a index that represent a word or a subword in the Bert vocabulary\n        attention_mask = tokens['attention_mask'] #0 is a padded value\n\n        targets.append([])\n        row_ids.append([])\n\n        label = np.array([EMPTY_IDX for _ in range(hyperparameters['max_length'])]) \n        offsets = tokens['offset_mapping'] #vector that retuns the starting character position and the end character position of each token\n\n        label_empty = True\n        for index, row in group_df.iterrows():\n            target = row.TARGET #this is the target label representing a unique feature that has to be detected in the patient history\n            targets[-1].append(target)\n            row_ids[-1].append(row.id)\n\n            for i, (w_start, w_end) in enumerate(offsets):\n                for start, end in decode_location(row.location): \n                    if w_start < w_end and (w_start >= start) and (end >= w_end): \n                        #if the position of a token in the offset mapping vector is included between the index positions indicated by the location feature\n                        #then assign the target label of the row that is considered to the label vector entry that matches the index position of that token \n                        label[i] = target\n                        label_empty = False\n                    if w_start >= w_end:\n                        break\n        if not label_empty:\n            sequences.append(sequence)\n            masks.append(attention_mask)\n            labels.append(label)\n            offsets_list.append(offsets)\n\n    sequences = np.array(sequences).astype(np.int32) #transform the list into an array\n    masks = np.array(masks).astype(np.uint8) \n    labels = F.one_hot(torch.Tensor(np.array(labels)).long(), num_classes=N_CLASSES) #one hot encoding of labels\n    labels = np.array(labels) #to transform the tensor back to an array \n    targets_to_row_ids = [dict(zip(a,b)) for a,b in zip(targets,row_ids)]\n    np.save(open(\"masks.npy\",'wb'), masks)\n    np.save(open(\"sequences.npy\",'wb'), sequences)\n    np.save(open(\"labels.npy\",'wb'), labels)\n    np.save(open(\"targets_to_row_ids.npy\", 'wb'), targets_to_row_ids)\n    np.save(open(\"offsets_list\", 'wb'), offsets_list)\nelse:\n    masks = np.load(open(DATA_PATH+\"/masks.npy\",'rb'))\n    sequences = np.load(open(DATA_PATH+\"/sequences.npy\",'rb'))\n    labels = np.load(open(DATA_PATH+\"/labels.npy\",'rb'))\n    targets_to_row_ids = np.load(open(DATA_PATH+\"/targets_to_row_ids.npy\", 'rb'),allow_pickle=True)\n    offsets_list = np.load(open(DATA_PATH+'/offsets_list', 'rb'))","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.318987Z","iopub.execute_input":"2022-12-29T21:05:40.319262Z","iopub.status.idle":"2022-12-29T21:05:40.530139Z","shell.execute_reply.started":"2022-12-29T21:05:40.319237Z","shell.execute_reply":"2022-12-29T21:05:40.529166Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"We create a new dataframe with only the input that is required by the Bert model:","metadata":{}},{"cell_type":"code","source":"train_bert = pd.DataFrame({'sequence': sequences.tolist(),'mask': masks.tolist(), 'label': labels.tolist()})","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:40.531684Z","iopub.execute_input":"2022-12-29T21:05:40.532057Z","iopub.status.idle":"2022-12-29T21:05:44.106860Z","shell.execute_reply.started":"2022-12-29T21:05:40.532019Z","shell.execute_reply":"2022-12-29T21:05:44.105861Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Validation Split","metadata":{}},{"cell_type":"markdown","source":"If both flag variables TRAIN and VAL are set to True then the training set is splitted into train and validation sets. If only TRAIN is true then the model is trained on all the dataset to achieve the best training before the testing phase. When both the flag variables TRAIN and VAL are set to False then that indicates it is the submission phase.","metadata":{}},{"cell_type":"code","source":"TRAIN = False\nVAL = False","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:44.108403Z","iopub.execute_input":"2022-12-29T21:05:44.108778Z","iopub.status.idle":"2022-12-29T21:05:44.114594Z","shell.execute_reply.started":"2022-12-29T21:05:44.108741Z","shell.execute_reply":"2022-12-29T21:05:44.112820Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"if VAL:\n    train_bert, val_bert = train_test_split(train_bert, test_size=hyperparameters['val_size'], random_state=hyperparameters['seed'])\n    targets_to_row_ids_train, targets_to_row_ids_val = np.take(targets_to_row_ids, train_bert.index), np.take(targets_to_row_ids, val_bert.index)\n    offsets_train, offsets_val =np.take(np.array(offsets_list), train_bert.index, axis = 0), np.take(np.array(offsets_list), val_bert.index, axis= 0)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:44.116326Z","iopub.execute_input":"2022-12-29T21:05:44.116703Z","iopub.status.idle":"2022-12-29T21:05:44.125050Z","shell.execute_reply.started":"2022-12-29T21:05:44.116668Z","shell.execute_reply":"2022-12-29T21:05:44.124355Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Define Dataset and Model classes","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    \n    def __init__(self, data, tokenizer, hyperparameters):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.hyperparameters = hyperparameters\n\n    def __len__(self):\n        '''Method that returns the length of the dataset'''\n        return len(self.data)\n\n    def __getitem__(self, index):\n        '''Method that processes and returns 1 datapoint at a time.'''\n        sequence = self.data.iloc[index][\"sequence\"]\n        mask = self.data.iloc[index]['mask']\n        label = self.data.iloc[index]['label']\n        return np.array(sequence), np.array(mask), np.array(label)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:44.126262Z","iopub.execute_input":"2022-12-29T21:05:44.127289Z","iopub.status.idle":"2022-12-29T21:05:44.137085Z","shell.execute_reply.started":"2022-12-29T21:05:44.127253Z","shell.execute_reply":"2022-12-29T21:05:44.136350Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, hyperparameters):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(hyperparameters['model_name']) # BERT model\n        self.dropout = nn.Dropout(p=hyperparameters['dropout'])\n        self.config = config\n        self.fc1 = nn.Linear(768, N_CLASSES) \n\n    def summary(self):\n        return summary(self)\n\n    def forward(self, input_ids, attention):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention)\n        logits = self.fc1(self.dropout(outputs[0]))\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:44.138314Z","iopub.execute_input":"2022-12-29T21:05:44.138978Z","iopub.status.idle":"2022-12-29T21:05:44.147457Z","shell.execute_reply.started":"2022-12-29T21:05:44.138942Z","shell.execute_reply":"2022-12-29T21:05:44.146580Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Training and validation functions","metadata":{}},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, criterion):\n    ''' \n    Function for training the model.\n        inputs : model, dataloader, optimizer, criterion\n        outputs : accuracy, precision, recall, f1_micro, train_loss\n    '''\n    model.train()\n\n    tp, tn, fp, fn, train_loss = 0, 0, 0, 0, 0\n    logits_list = []\n    for batch in tqdm(dataloader):\n\n        optimizer.zero_grad()\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n        logits = model(input_ids, attention_mask)\n        \n        loss = criterion(torch.permute(logits, (0,2,1)),torch.argmax(labels, dim = 2))\n        predicted = torch.argmax(logits,dim =  2).detach().cpu().numpy()\n        labelled = torch.argmax(labels,dim = 2).detach().cpu().numpy()\n\n        tp +=  (np.sum((predicted == labelled) & (predicted != 0)))\n        tn += (np.sum((predicted == labelled) & (predicted == 0)))\n        fp +=  (np.sum((predicted != 0) & (labelled == 0)))\n        fn += (np.sum((predicted == 0) & (labelled != 0)))\n        train_loss += (loss.item() * input_ids.size(0))\n\n        loss.backward()\n\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0) #clipping the gradient to avoid exploding gradients\n        optimizer.step()\n\n    accuracy = ((tp+tn)/(hyperparameters['max_length']*len(train_bert)))*100\n    try:\n        precision = (tp /(tp+fp))\n    except:\n        precision = 0.0 \n    recall = tp/(tp+fn)\n    f1_micro = tp/(tp+0.5*(fp+fn))\n    train_loss = train_loss/len(train_bert)\n    return accuracy, precision, recall, f1_micro, train_loss","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:44.148752Z","iopub.execute_input":"2022-12-29T21:05:44.149314Z","iopub.status.idle":"2022-12-29T21:05:44.162331Z","shell.execute_reply.started":"2022-12-29T21:05:44.149261Z","shell.execute_reply":"2022-12-29T21:05:44.161240Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, dataloader, criterion):\n\n    model.eval()\n\n    tp, tn, fp, fn, val_loss  = 0, 0, 0, 0, 0\n    logits_list = []\n\n    for batch in tqdm(dataloader): \n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n        logits = model(input_ids, attention_mask)\n\n        loss = criterion(torch.permute(logits, (0,2,1)),torch.argmax(labels, dim = 2))\n        predicted = torch.argmax(logits,dim =  2).detach().cpu().numpy()\n        labelled = torch.argmax(labels,dim = 2).detach().cpu().numpy()\n\n        tp +=  (np.sum((predicted == labelled) & (predicted != 0)))\n        tn += (np.sum((predicted == labelled) & (predicted == 0)))\n        fp +=  (np.sum((predicted != 0) & (labelled == 0)))\n        fn += (np.sum((predicted == 0) & (labelled != 0)))\n        val_loss += (loss.item() * input_ids.size(0))\n\n    try:\n        precision = (tp /(tp+fp))\n    except:\n        precision = 0.0 \n    recall = tp/(tp+fn)\n    f1_micro = tp/(tp+0.5*(fp+fn))\n    val_loss = val_loss/len(val_bert)\n    return accuracy, precision, recall, f1_micro, val_loss","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:44.163911Z","iopub.execute_input":"2022-12-29T21:05:44.164364Z","iopub.status.idle":"2022-12-29T21:05:44.176476Z","shell.execute_reply.started":"2022-12-29T21:05:44.164325Z","shell.execute_reply":"2022-12-29T21:05:44.175729Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"markdown","source":"To create a dataframe to test the results:","metadata":{}},{"cell_type":"code","source":"df_old = pd.DataFrame([['','','', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], columns = ['batch_size', 'lr', 'epochs', 'train_acc', 'val_acc', 'train_loss', 'val_loss', 'train_prec', 'val_prec', 'train_rec', 'val_rec', 'train_f1', 'val_f1'])","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:05:44.177666Z","iopub.execute_input":"2022-12-29T21:05:44.178656Z","iopub.status.idle":"2022-12-29T21:05:44.190783Z","shell.execute_reply.started":"2022-12-29T21:05:44.178619Z","shell.execute_reply":"2022-12-29T21:05:44.189831Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"The nested for-loops have been created just to test all possible combinations of hyperparameters. The best model is then saved. When the final training with all data is needed to be run then the loop can be commented out and the batch_size, epochs and lr set to the \"right\" values.","metadata":{}},{"cell_type":"code","source":"device = \"cuda\"\nmodel = CustomModel(hyperparameters).to(device)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:07:17.393463Z","iopub.execute_input":"2022-12-29T21:07:17.393881Z","iopub.status.idle":"2022-12-29T21:07:19.010232Z","shell.execute_reply.started":"2022-12-29T21:07:17.393844Z","shell.execute_reply":"2022-12-29T21:07:19.009105Z"},"scrolled":true,"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"CustomModel(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc1): Linear(in_features=768, out_features=144, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"if TRAIN:\n    \n    for comb in range(3):\n        for comb_2 in range(3):\n            for size in range(2):\n                \n                training_data = CustomDataset(train_bert, tokenizer, hyperparameters)\n                train_dataloader = DataLoader(training_data, batch_size=hyperparameters['batch_size'][size], shuffle=True, worker_init_fn=seed_worker,generator=g)\n\n                # Define the loss function\n                criterion = nn.CrossEntropyLoss()\n\n                # Define the optimizer\n                optimizer = optim.AdamW(model.parameters(), lr=hyperparameters['lr'][comb])\n\n\n                if VAL:\n                    val_data = CustomDataset(val_bert, tokenizer, hyperparameters)\n                    val_dataloader = DataLoader(val_data, batch_size=hyperparameters['batch_size'][size], shuffle=False, worker_init_fn=seed_worker,generator=g)\n                    val_loss_min = np.Inf\n                    best_loss = np.inf\n                    \n                since = time.time()\n                epochs = hyperparameters['epochs'][comb_2]\n                train_acc, val_acc, train_loss, val_loss, train_prec, val_prec, train_rec, val_rec, train_f1, val_f1 = list(), list(), list(), list(), list(), list(), list(), list(), list(), list()\n\n                for i in range(epochs):\n\n                    print(\"Epoch: {}/{}\".format(i + 1, epochs))\n                    #TRAIN THE MODEL \n                    t_acc, t_prec, t_rec, t_f1, t_loss = train_model(model, train_dataloader, optimizer, criterion) # t_prec, t_rec, t_f1\n                    train_acc.append(t_acc)\n                    train_loss.append(t_loss)\n                    train_prec.append(float(t_prec))\n                    train_rec.append(float(t_rec))\n                    train_f1.append(float(t_f1))\n                    print(f\"TRAINING. Loss: {t_loss:.2f};, Accuracy: {t_acc:.2f}; Precision: {t_prec:.2f}; Recall: {t_rec:.2f}; Micro-F1 score: {t_f1:.2f}\")\n                    #VALIDATE THE MODEL\n                    if VAL:\n                        v_acc, v_prec, v_rec, v_f1, v_loss = eval_model(model, val_dataloader, criterion)\n                        val_acc.append(v_acc)\n                        val_loss.append(v_loss)\n                        val_prec.append(float(v_prec))\n                        val_rec.append(float(v_rec))\n                        val_f1.append(float(v_f1))\n                        print(f\"VALIDATION. Loss: {v_loss:.2f};, Accuracy: {v_acc:.2f}; Precision: {v_prec:.2f}; Recall: {v_rec:.2f}; Micro-F1 score: {v_f1:.2f}\")\n\n                        if v_loss < best_loss: #to save the model with best validation loss\n                            best_loss = v_loss\n                            torch.save(model.state_dict(), \"nbme_bert_v2.pth\")\n\n                time_elapsed = time.time() - since\n                print('Training completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n\n\n                if VAL:\n                    \n                    #save the results to a dataframe\n                    df = pd.DataFrame([[str(hyperparameters['batch_size'][size]), str(hyperparameters['lr'][comb]), str(hyperparameters['epochs'][comb_2]), train_acc[-1], val_acc[-1], train_loss[-1], val_loss[-1], train_prec[-1], val_prec[-1], train_rec[-1], val_rec[-1], train_f1[-1], val_f1[-1]]], columns = ['batch_size', 'lr', 'epochs', 'train_acc', 'val_acc', 'train_loss', 'val_loss', 'train_prec', 'val_prec', 'train_rec', 'val_rec', 'train_f1', 'val_f1'])\n                    df_old = pd.concat([df_old, df], join = 'inner')\n                    df_old = df_old.sort_values(by = 'val_f1',ascending=False)\n                    df_old.to_csv('df.csv')\n                    \n                    # Plot the results\n                    plt.plot(train_loss, \"-o\", label='Training loss')\n                    plt.plot(val_loss, \"-o\", label='Validation loss')\n                    plt.title('Training and validation loss')\n                    plt.xlabel('epochs')\n                    plt.ylabel('loss')\n                    plt.legend()\n                    plt.savefig(f'loss.png', dpi=300)\n                    plt.figure()\n\n                    plt.plot(train_acc, \"-o\", label='Training accuracy')\n                    plt.plot(val_acc, \"-o\", label='Validation accuracy')\n                    plt.title('Training and validation accuracy')\n                    plt.xlabel('epochs')\n                    plt.ylabel('accuracy')\n                    plt.legend()\n                    plt.savefig(f'acc.png', dpi=300)\n                    plt.figure()\n\n                    plt.plot(train_prec, \"-o\", label='Training precision')\n                    plt.plot(val_prec, \"-o\", label='Validation precision')\n                    plt.title('Training and validation precision')\n                    plt.xlabel('epochs')\n                    plt.ylabel('precision')\n                    plt.legend()\n                    plt.savefig(f'prec.png', dpi=300)\n                    plt.figure()\n\n                    plt.plot(train_rec, \"-o\", label='Training recall')\n                    plt.plot(val_rec, \"-o\", label='Validation recall')\n                    plt.title('Training and validation recall')\n                    plt.xlabel('epochs')\n                    plt.ylabel('recall')\n                    plt.legend()\n                    plt.savefig(f'recall.png', dpi=300)\n                    plt.figure()\n\n                    plt.plot(train_f1, \"-o\", label='Training F1-Score')\n                    plt.plot(val_f1, \"-o\", label='Validation F1-Score')\n                    plt.title('Training and validation F1-Score')\n                    plt.xlabel('epochs')\n                    plt.ylabel('f1-score')\n                    plt.legend()\n                    plt.savefig(f'f1score.png', dpi=300)\n                    plt.figure()\n                    \n                else: #if training on the all dataset without validating\n                    torch.save(model.state_dict(), \"new_model.pth\")\nelse: #testing phase\n    model.load_state_dict(torch.load(DATA_PATH + \"/new_model.pth\", map_location = device))","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:07:48.684837Z","iopub.execute_input":"2022-12-29T21:07:48.685226Z","iopub.status.idle":"2022-12-29T21:07:48.973566Z","shell.execute_reply.started":"2022-12-29T21:07:48.685192Z","shell.execute_reply":"2022-12-29T21:07:48.972349Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"markdown","source":"##### Creating the ids vector, the attention mask vector and the offset mapping vector for the test set:","metadata":{}},{"cell_type":"code","source":"test_sequences, test_masks, test_offsets = [], [],[]\nrow_ids = []\ntargets = []\n\nfor g1 in tqdm(test.groupby('pn_num')):\n    gdf = g1[1]\n    pn_history  = gdf.iloc[0].pn_history\n    targets.append([])\n    row_ids.append([])\n\n    test_tokens = tokenizer.encode_plus(pn_history, max_length=hyperparameters['max_length'], padding='max_length',truncation=True, return_offsets_mapping=True)\n    test_sequence = test_tokens['input_ids']\n    test_attention_mask = test_tokens['attention_mask'] \n\n    # BUILD THE TARGET ARRAY\n    offset = test_tokens['offset_mapping']\n    for index, row in gdf.iterrows():\n        targets[-1].append(row.TARGET)\n        row_ids[-1].append(row.id)\n\n    test_sequences.append(test_sequence)\n    test_masks.append(test_attention_mask)\n    test_offsets.append(offset)\n\ntest_sequences = np.array(test_sequences).astype(np.int32)\ntest_masks = np.array(test_masks).astype(np.uint8)\ntargets_to_row_ids_test = [dict(zip(a,b)) for a,b in zip(targets,row_ids)]\ntest_bert = pd.DataFrame({'sequence': test_sequences.tolist(),'mask': test_masks.tolist()})","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:07:54.622741Z","iopub.execute_input":"2022-12-29T21:07:54.623127Z","iopub.status.idle":"2022-12-29T21:07:54.676444Z","shell.execute_reply.started":"2022-12-29T21:07:54.623095Z","shell.execute_reply":"2022-12-29T21:07:54.675500Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728e9ea390c14d6c8f6dea3a8ef16383"}},"metadata":{}}]},{"cell_type":"markdown","source":"##### The SubmissionDataset class is similar to the CustomDataset class except that it does not return any ground truth in the \\__geitem\\__ method. ","metadata":{}},{"cell_type":"code","source":"class SubmissionDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        '''Function that processes and returns 1 datapoint at a time.'''\n        sequence = self.data.iloc[index][\"sequence\"]\n        mask = self.data.iloc[index]['mask']\n        return np.array(sequence), np.array(mask)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:07:54.703622Z","iopub.execute_input":"2022-12-29T21:07:54.704605Z","iopub.status.idle":"2022-12-29T21:07:54.711168Z","shell.execute_reply.started":"2022-12-29T21:07:54.704568Z","shell.execute_reply":"2022-12-29T21:07:54.709837Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"submission_data = SubmissionDataset(test_bert, tokenizer, hyperparameters)\nsubmission_dataloader = DataLoader(submission_data, batch_size=4, shuffle=False, worker_init_fn=seed_worker,generator=g)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:07:55.284151Z","iopub.execute_input":"2022-12-29T21:07:55.284520Z","iopub.status.idle":"2022-12-29T21:07:55.291667Z","shell.execute_reply.started":"2022-12-29T21:07:55.284488Z","shell.execute_reply":"2022-12-29T21:07:55.290505Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\nlogits_list = []\nval_loss = 0\n\nfor batch in tqdm(submission_dataloader): \n    input_ids = batch[0].to(device)\n    attention_mask = batch[1].to(device)\n    logits = model(input_ids, attention_mask)\n    logits_list.append(logits.detach().cpu().numpy())\n\nlogits = np.concatenate(logits_list, axis = 0)\npreds = np.argmax((logits), -1) #to transform it from one-hot encoding predictions to numerical predicitons from 0 to 143","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:07:55.725841Z","iopub.execute_input":"2022-12-29T21:07:55.727588Z","iopub.status.idle":"2022-12-29T21:07:55.971651Z","shell.execute_reply.started":"2022-12-29T21:07:55.727545Z","shell.execute_reply":"2022-12-29T21:07:55.970553Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a391180f9e844289a8ac6287cc410946"}},"metadata":{}}]},{"cell_type":"markdown","source":"The predictions vector output of the model has the first dimension equal to the number of unique patient history in the test set and the second dimension, after applying np.argmax(),  equals the number of unique features, 144. So the translate() function is applied in order to take the prediction vectors and to output the corresponding location of every target feature in the test set. Every row is a different feature to be searched and many rows share the same patient history text. Since this model outputs the predictions of different features all concatenated toghether in the same vector, the translate() function splits the prediction vector into the format required by the competition.","metadata":{}},{"cell_type":"code","source":"sub = translate(preds.reshape(len(preds),512),targets_to_row_ids_test,test_offsets).sort_values('id')\n\nsub.to_csv('submission.csv',index=False)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-29T21:07:56.630224Z","iopub.execute_input":"2022-12-29T21:07:56.630609Z","iopub.status.idle":"2022-12-29T21:07:56.645795Z","shell.execute_reply.started":"2022-12-29T21:07:56.630576Z","shell.execute_reply":"2022-12-29T21:07:56.644700Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"          id location\n0  00016_000  696 724\n1  00016_001  668 693\n2  00016_002  203 217\n3  00016_003    70 91\n4  00016_004         ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>location</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00016_000</td>\n      <td>696 724</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00016_001</td>\n      <td>668 693</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00016_002</td>\n      <td>203 217</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00016_003</td>\n      <td>70 91</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00016_004</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# CREDITS:\n","metadata":{}},{"cell_type":"markdown","source":"The general idea for the architecture of the NER classifier, as well as the code to create the groundtruth vector and the ausiliary functions to translate the output of the model to the same format required by the competition were taken from the Kaggle user *tchaye59*. His code was of course re-adapted as it was developed using Tensorflow while we used PyTorch. The structure of the training loop and the CustomDataset and CustomModel class were readapted by the code of *iamsdt*. The respective notebooks links follow:\n\nhttps://www.kaggle.com/code/tchaye59/nbme-tensorflow-bert-baseline\n\nhttps://www.kaggle.com/code/iamsdt/pytorch-bert-baseline-nbme","metadata":{}}]}